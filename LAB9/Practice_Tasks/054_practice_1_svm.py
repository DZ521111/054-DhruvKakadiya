# -*- coding: utf-8 -*-
"""054_Practice_1-SVM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RZgwZ5Yc1Gz6m02_sbDQvuipH6KljwDZ

#Aim: To implement SVM using scikit-learn library and train it to classify Breast Cancer Data.

##Key Terms: 

**Hyperplane:** A hyperplane is a decision plane which separates between a set of objects having different class memberships.

**Support Vectors :** Support vectors are the data points, which are closest to the hyperplane. These points will define the separating line better by calculating margins.

**Margin :** A margin is a gap between the two lines on the closest class points. This is calculated as the perpendicular distance from the line to support vectors or closest points. If the margin is larger in between the classes, then it is considered a good margin, a smaller margin is a bad margin.

**SVM Kernel :** The SVM algorithm is implemented in practice using a kernel. A kernel transforms an input data space into the required form.

**Linear Kernel :** A linear kernel can be used as normal dot product any two given observations. The product between two vectors is the sum of the multiplication of each pair of input values.

$ K(x, xi) = sum(x * xi) $

**Polynomial Kernel :** A polynomial kernel is a more generalized form of the linear kernel. The polynomial kernel can distinguish curved or nonlinear input space.
$ K(x,xi) = 1 + sum(x * xi)^d$

**RBF (Radial Basis Function) Kernel :** The Radial basis function kernel is a popular kernel function commonly used in support vector machine classification.RBF can map an input space in infinite dimensional space.
$ K(x,xi) = exp(-gamma * sum((x xi^2)) $

Here gamma is a parameter, which ranges from 0 to 1. A higher value of gamma will perfectly fit the training dataset, which causes over-fitting. Gamma=0.1 is considered to be a good default value. The value of gamma needs to be manually specified in the learning algorithm.

**Part A: Basic SVM with Linear Kernel**
"""

import sys, os
import matplotlib.pyplot as plt
from sklearn import svm
from sklearn.model_selection import train_test_split
import numpy as np

# importing scikit learn with make_blobs
from sklearn.datasets.samples_generator import make_blobs
# creating datasets X containing n_samples
# Y containing two classes
X, Y = make_blobs(n_samples=500, centers=2, random_state = 54, cluster_std=0.40)
# plotting scatters
plt.scatter(X[:, 0], X[:, 1], c=Y, s=50, cmap='spring');
plt.show()
# Split data to train and test on 80-20 ratio
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2,random_state=0)

# Create a linear SVM classifier

lsvmc = svm.SVC(kernel='linear')

# Train classifier

lsvmc.fit(X,Y)

## Plot decision function on training and test data
#plot_decision_function(X_train, y_train, X_test, y_test, clf)

# Make predictions on unseen test data
clf_predictions = lsvmc.predict(X_test)
print("Accuracy: {}%".format(lsvmc.score(X_test, y_test) * 100 ))

def make_meshgrid(x, y, h=.02):
  x_min, x_max = x.min() - 1, x.max() + 1
  y_min, y_max = y.min() - 1, y.max() + 1
  xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
  return xx, yy

def plot_contours(ax, clf, xx, yy, **params):
  Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
  Z = Z.reshape(xx.shape)
  out = ax.contourf(xx, yy, Z, **params)
  return out
  
fig, ax = plt.subplots()
# title for the plots
title = ('Decision surface of linear SVC ')
# Set-up grid for plotting.
X0, X1 = X[:, 0], X[:, 1]
xx, yy = make_meshgrid(X0, X1)
plot_contours(ax, lsvmc, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)
ax.scatter(X0, X1, c=Y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')
ax.set_ylabel('y label here')
ax.set_xlabel('x label here')
ax.set_xticks(())
ax.set_yticks(())
ax.set_title(title)
ax.legend()
plt.show()

"""**Part B : Breast Cancer Prediction Example**"""

#Import scikit-learn dataset library
from sklearn import datasets
#Load dataset
cancer = datasets.load_breast_cancer()

# print the names of the 13 features
print("Features: ", cancer.feature_names)
# print the label type of cancer('malignant' 'benign')
print("Labels: ", cancer.target_names)

# print data(feature)shape
cancer.data.shape

# print the cancer labels (0:malignant, 1:benign)
print(cancer.target)

# plotting scatters
plt.scatter(cancer.data[:, 0], cancer.data[:, 1], c=cancer.target, s=50,cmap='spring');
plt.show()

# Import train_test_split function
from sklearn.model_selection import train_test_split
# Split dataset into training set and test set
X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size = 0.2,random_state = 54)

#Import svm model
from sklearn import svm
#Create a svm Classifier
lsvmc = svm.SVC(kernel='linear') # Linear Kernel
#Train the model using the training sets
lsvmc.fit(X_train,y_train)
#Predict the response for test dataset
y_pred = lsvmc.predict(X_test)

#Import scikit-learn metrics module for accuracy calculation
from sklearn import metrics
# Model Accuracy: how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

# Model Precision: what percentage of positive tuples are labeled as such?
print("Precision:",metrics.precision_score(y_test, y_pred))
# Model Recall: what percentage of positive tuples are labelled as such?
print("Recall:",metrics.recall_score(y_test, y_pred))

"""**Assignment**:

Try SVM classifier on MNIST dataset, compare the preformance of linear, polynomial and RBF kernels.

"""